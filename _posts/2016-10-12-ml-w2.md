---
layout: post
title: Machine Learning and Data Analysis - Week2 | Random Forest
published: true
---

This is the second assignment of the fourth course (of five)
[Data Analysis and Interpretation Specialization](https://www.coursera.org/specializations/data-analysis)
detailed information about it can be seeing [here](https://www.coursera.org/learn/machine-learning-data-analysis).


### Index
+ [Variables](#variables)
+ [Confusion Matrix](#matrix)
+ [Relative Importance of Atributes](#importance)
+ [Random Forest](#rf)
  + [Accuracy](#acc)
+ [Conclusion](#conclusion)  

### <a name = "variables"></a>Variables

Details of my project can be seeing
[here](https://sidon.github.io/data-visualization-week1/), to get easier, I made a summary bellow:

|Variable Name|Description|
|-------------|-----------|
|Income       |Explanatory Variable: GPD per capita (1)|
|Alcohol      |Explanatory Variable:: Alcohol Consumption (3)|
|Life         |Response Variable: Life Expectancy (2)|

(1) 2010 Gross Domestic Product per capita in constant 2000 US$"

(2) 2011 life expectancy at birth (years)

(3) 2008 alcohol consumption per adult (liters, age 15+)

### <a name = "matrix"></a>Confusion Matrix
The figure bellow represents the confusion matrix, for see the code that generated
the matrix and this graph, visite the section 11 of
[this notebook](https://github.com/Sidon/Sidon.github.io/blob/master/_posts/rforest.ipynb)

![plot](/images/matrix2.png)

This graph represents the confusion matrix, that shows the correct and incorrect classifications of the decision tree, the labels means the life expectancy greater and less than 69 years (class 0 and class 1), the horizontal labels are the predictions and the vertical are the reference (true). the main diagonal: 18 and 32 represent the number of true negative and true positive for life expectancy and the other diagonal (values 9 and 10) represents the numbers of false negative and false positive respectively.

### <a name = "importance"></a>Relative Importance of Atributes

The code fragment bellow prints the relative importance of acohol and income respectively:

```python
# fit an Extra Trees model to the data
model = ExtraTreesClassifier()
model.fit(pred_train,tar_train)
# display the relative importance of each attribute
print(model.feature_importances_)

[ 0.40728949  0.59271051]
```
See the entire code for this week [on this jupyter notebook](https://github.com/Sidon/Sidon.github.io/blob/master/_posts/reforest.ipynb)

As I have only 2 explanatory variables, only one can be considered as having highest relative importance scores and that is Income.

### <a name = "rf"></a>Random Forest

I Runned a diferrent number off trees (1 to 25) and plot a graph:
![plot](/images/rf-trees3.png)


#### <a name = "acc"></a>Accuracy
The maximium accuracy on randdom forest was 0.72 against 0.71 on single tree.
Bellow the fragment code for both is showed:

```python
#Build model on training data for decision tree
clfdt = DecisionTreeClassifier()
clfdt = clfdt.fit(pred_train,tar_train)
preddt=clfdt.predict(pred_test)

accuracy = sklearn.metrics.accuracy_score(tar_test, preddt)
print ('Accuracy Score for Decision Tree: ', accuracy,'\n')

Accuracy Score for Decision Tree:  0.710144927536

accuracy = sklearn.metrics.accuracy_score(tar_test, predictions)
print ('Accuracy Score for Random Forest: ', accuracy,'\n')

Accuracy Score for Random Forest:  0.724637681159
```

### <a name = "conclusion"></a>Conclusion
The best result of the random forest after 25 trees is 72% while a single tree resulted in 71%, this is very little for be considered significant, then we can conclude that interpretation of a single decision tree may be appropriate.


See the entire code and output for this week,  [here](https://github.com/Sidon/Sidon.github.io/blob/master/_posts/rforest.ipynb).
